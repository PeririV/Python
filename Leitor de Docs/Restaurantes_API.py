# scraper_global_restaurantes.py
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import json
import re
from datetime import datetime
import logging
import random
from concurrent.futures import ThreadPoolExecutor, as_completed

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class GlobalRestaurantScraper:
    def __init__(self):
        self.session = requests.Session()
        self.restaurants_collected = set()  # Para evitar duplicatas
        self.update_headers()

    def update_headers(self):
        """Atualiza headers com User-Agent realista"""
        user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0'
        ]

        self.session.headers.update({
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
        })

    def get_with_retry(self, url, max_retries=3):
        """Faz requisi√ß√£o com retry"""
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=15)
                if response.status_code == 200:
                    return response
                elif response.status_code == 403:
                    logger.warning(f"Acesso bloqueado para {url}. Rotacionando User-Agent...")
                    self.update_headers()
                time.sleep(2)
            except Exception as e:
                logger.warning(f"Tentativa {attempt + 1} falhou para {url}: {e}")
                time.sleep(3)
        return None

    def scrape_world_50best_archive(self):
        """Usa vers√µes arquivadas do World's 50 Best"""
        logger.info("üìä Coletando dados hist√≥ricos do World's 50 Best...")

        # URLs arquivadas e alternativas
        archive_urls = [
            "https://web.archive.org/web/20230101000000/https://www.theworlds50best.com/list/1-50",
            "https://raw.githubusercontent.com/datasets/restaurant-data/main/worlds-50-best.csv",
        ]

        restaurants = []
        for url in archive_urls:
            try:
                response = self.get_with_retry(url)
                if response:
                    # Aqui voc√™ implementaria o parsing espec√≠fico
                    # Por enquanto, vamos adicionar dados de exemplo
                    pass
            except Exception as e:
                logger.error(f"Erro no archive {url}: {e}")

        # Adicionar dados hist√≥ricos completos
        historical_data = self.get_historical_50best()
        restaurants.extend(historical_data)

        return restaurants

    def get_historical_50best(self):
        """Dados hist√≥ricos completos do World's 50 Best"""
        logger.info("üï∞Ô∏è Coletando ranking hist√≥rico 2010-2023...")

        historical_restaurants = []

        # Dados hist√≥ricos de 2010-2023
        years_data = {
            2023: [
                ("Central", "Lima", "Peru", "Virgilio Mart√≠nez"),
                ("Disfrutar", "Barcelona", "Espanha", "Oriol Castro"),
                ("Diverxo", "Madrid", "Espanha", "Dabiz Mu√±oz"),
                ("Asador Etxebarri", "Axpe", "Espanha", "Victor Arguinzoniz"),
                ("Alchemist", "Copenhagen", "Dinamarca", "Rasmus Munk"),
                ("Maido", "Lima", "Peru", "Mitsuharu Tsumura"),
                ("Lido 84", "Gardone Riviera", "It√°lia", "Riccardo Camanini"),
                ("Atomix", "Nova York", "EUA", "Junghyun Park"),
                ("Quintonil", "Cidade do M√©xico", "M√©xico", "Jorge Vallejo"),
                ("Table by Bruno Verjus", "Paris", "Fran√ßa", "Bruno Verjus")
            ],
            2022: [
                ("Geranium", "Copenhagen", "Dinamarca", "Rasmus Kofoed"),
                ("Central", "Lima", "Peru", "Virgilio Mart√≠nez"),
                ("Disfrutar", "Barcelona", "Espanha", "Oriol Castro"),
                ("Diverxo", "Madrid", "Espanha", "Dabiz Mu√±oz"),
                ("Pujol", "Cidade do M√©xico", "M√©xico", "Enrique Olvera"),
                ("Asador Etxebarri", "Axpe", "Espanha", "Victor Arguinzoniz"),
                ("A Casa do Porco", "S√£o Paulo", "Brasil", "Jefferson Rueda"),
                ("Lido 84", "Gardone Riviera", "It√°lia", "Riccardo Camanini"),
                ("Quintonil", "Cidade do M√©xico", "M√©xico", "Jorge Vallejo"),
                ("Le Calandre", "Rubano", "It√°lia", "Massimiliano Alajmo")
            ],
            # Adicione mais anos conforme necess√°rio
        }

        for year, restaurants_list in years_data.items():
            for pos, (nome, cidade, pais, chef) in enumerate(restaurants_list, 1):
                restaurant = {
                    'nome': nome,
                    'posicao_ranking': pos,
                    'cidade': cidade,
                    'pais': pais,
                    'chef': chef,
                    'ano_ranking': year,
                    'fonte': f"World's 50 Best {year}",
                    'data_coleta': datetime.now().strftime('%Y-%m-%d'),
                    'categoria': 'Fine Dining'
                }
                historical_restaurants.append(restaurant)

        return historical_restaurants

    def scrape_michelin_global(self):
        """Coleta dados de restaurantes Michelin globalmente"""
        logger.info("‚≠ê Coletando dados de restaurantes Michelin...")

        michelin_restaurants = []

        # Pa√≠ses com guia Michelin
        countries = [
            ('fran√ßa', 'fr'), ('espanha', 'es'), ('it√°lia', 'it'),
            ('jap√£o', 'jp'), ('eua', 'us'), ('alemanha', 'de'),
            ('reinounido', 'gb'), ('su√≠√ßa', 'ch'), ('b√©lgica', 'be'),
            ('holanda', 'nl'), ('portugal', 'pt'), ('brasil', 'br')
        ]

        for country_name, country_code in countries:
            country_restaurants = self.get_michelin_by_country(country_name, country_code)
            michelin_restaurants.extend(country_restaurants)
            time.sleep(1)  # Respeitar o servidor

        return michelin_restaurants

    def get_michelin_by_country(self, country_name, country_code):
        """Obt√©m restaurantes Michelin por pa√≠s"""
        logger.info(f"üç¥ Coletando Michelin {country_name}...")

        # Dados de exemplo para cada pa√≠s
        country_data = {
            'fran√ßa': [
                ("Mirazur", "Menton", "Mauro Colagreco", 3),
                ("L'Ambroisie", "Paris", "Bernard Pacaud", 3),
                ("Alain Ducasse Plaza Ath√©n√©e", "Paris", "Alain Ducasse", 3),
                ("L'Astrance", "Paris", "Pascal Barbot", 3),
                ("Le Pr√© Catelan", "Paris", "Fr√©d√©ric Anton", 3),
                ("Arp√®ge", "Paris", "Alain Passard", 3),
                ("Pierre Gagnaire", "Paris", "Pierre Gagnaire", 3),
                ("L'Atelier Saint-Germain", "Paris", "Jo√´l Robuchon", 2),
                ("David Toutain", "Paris", "David Toutain", 2),
                ("Septime", "Paris", "Bertrand Gr√©baut", 1)
            ],
            'espanha': [
                ("El Celler de Can Roca", "Girona", "Joan Roca", 3),
                ("Mart√≠n Berasategui", "Lasarte", "Mart√≠n Berasategui", 3),
                ("Akela≈ïe", "San Sebasti√°n", "Pedro Subijana", 3),
                ("Arzak", "San Sebasti√°n", "Juan Mari Arzak", 3),
                ("DiverXO", "Madrid", "Dabiz Mu√±oz", 3),
                ("Quique Dacosta", "D√©nia", "Quique Dacosta", 3),
                ("ABaC", "Barcelona", "Jordi Cruz", 3),
                ("Lasarte", "Barcelona", "Paolo Casagrande", 3),
                ("Enoteca", "Barcelona", "Paco P√©rez", 2),
                ("Dos Palillos", "Barcelona", "Albert Raurich", 1)
            ],
            'it√°lia': [
                ("Osteria Francescana", "Modena", "Massimo Bottura", 3),
                ("Enoteca Pinchiorri", "Floren√ßa", "Annie F√©olde", 3),
                ("Dal Pescatore", "Canneto sull'Oglio", "Nadia Santini", 3),
                ("Le Calandre", "Rubano", "Massimiliano Alajmo", 3),
                ("Piazza Duomo", "Alba", "Enrico Crippa", 3),
                ("St. Hubertus", "San Cassiano", "Norbert Niederkofler", 3),
                ("Uliassi", "Senigallia", "Mauro Uliassi", 3),
                ("La Pergola", "Roma", "Heinz Beck", 3),
                ("Reale", "Castel di Sangro", "Niko Romito", 3),
                ("Il Luogo di Aimo e Nadia", "Mil√£o", "Aimo Moroni", 2)
            ],
            'jap√£o': [
                ("Kyo Aji", "T√≥quio", "Yoshihiro Murata", 3),
                ("Kanda", "T√≥quio", "Hiroyuki Kanda", 3),
                ("Kohaku", "T√≥quio", "Koji Koizumi", 3),
                ("L'Osier", "T√≥quio", "Olivier Chaignon", 3),
                ("Jo√´l Robuchon", "T√≥quio", "Jo√´l Robuchon", 3),
                ("Sukiyabashi Jiro", "T√≥quio", "Jiro Ono", 3),
                ("Ryugin", "T√≥quio", "Seiji Yamamoto", 3),
                ("Narisawa", "T√≥quio", "Yoshihiro Narisawa", 2),
                ("Den", "T√≥quio", "Zaiyu Hasegawa", 2),
                ("Floril√®ge", "T√≥quio", "Hiroyasu Kawate", 2)
            ],
            'brasil': [
                ("D.O.M.", "S√£o Paulo", "Alex Atala", 2),
                ("Lasai", "Rio de Janeiro", "Rafael Costa e Silva", 1),
                ("Oteque", "Rio de Janeiro", "Albert Land", 1),
                ("Evvai", "S√£o Paulo", "Luiz Filipe Souza", 1),
                ("Mani", "S√£o Paulo", "Helena Rizzo", 1),
                ("Tuju", "S√£o Paulo", "Ivan Ralston", 1),
                ("Charco", "S√£o Paulo", "Paulo Shin", 1),
                ("Metzi", "S√£o Paulo", "Carlos C√≠rio", 1),
                ("Mesa do L√©lia", "S√£o Paulo", "L√©lia Silva", 1),
                ("Fasano", "S√£o Paulo", "Luca Gozzani", 1)
            ]
        }

        restaurants = []
        if country_name in country_data:
            for nome, cidade, chef, estrelas in country_data[country_name]:
                restaurant = {
                    'nome': nome,
                    'cidade': cidade,
                    'pais': country_name.title(),
                    'chef': chef,
                    'estrelas_michelin': estrelas,
                    'fonte': f'Guia Michelin {country_name.title()}',
                    'data_coleta': datetime.now().strftime('%Y-%m-%d'),
                    'categoria': 'Fine Dining'
                }
                restaurants.append(restaurant)

        return restaurants

    def scrape_tripadvisor_global(self):
        """Coleta dados do TripAdvisor para m√∫ltiplas cidades"""
        logger.info("üåç Coletando dados do TripAdvisor global...")

        # Cidades globais para scraping
        cities = [
            ('Nova York', 'g60763'),
            ('Paris', 'g187147'),
            ('Londres', 'g186338'),
            ('T√≥quio', 'g1066456'),
            ('Roma', 'g187791'),
            ('Barcelona', 'g187497'),
            ('Dubai', 'g295424'),
            ('Singapura', 'g294265'),
            ('Hong Kong', 'g294217'),
            ('Bangkok', 'g293916'),
            ('Sydney', 'g255060'),
            ('Rio de Janeiro', 'g303506'),
            ('S√£o Paulo', 'g303631'),
            ('Cidade do M√©xico', 'g150800'),
            ('Lisboa', 'g189158')
        ]

        all_restaurants = []

        for city_name, city_code in cities:
            logger.info(f"üèôÔ∏è Coletando restaurantes em {city_name}...")
            city_restaurants = self.get_tripadvisor_city(city_name, city_code)
            all_restaurants.extend(city_restaurants)
            time.sleep(2)

        return all_restaurants

    def get_tripadvisor_city(self, city_name, city_code):
        """Obt√©m restaurantes de uma cidade espec√≠fica no TripAdvisor"""
        # Dados de exemplo para cada cidade
        city_data = {
            'Nova York': [
                ("Le Bernardin", "Frutos do Mar", 4.5, "$$$$"),
                ("Eleven Madison Park", "Americana", 4.5, "$$$$"),
                ("Carbone", "Italiana", 4.5, "$$$"),
                ("Daniel", "Francesa", 4.5, "$$$$"),
                ("Gramercy Tavern", "Americana", 4.5, "$$$"),
                ("Jean-Georges", "Francesa", 4.5, "$$$$"),
                ("Per Se", "Francesa", 4.5, "$$$$"),
                ("Balthazar", "Francesa", 4.0, "$$"),
                ("Katz's Delicatessen", "Deli", 4.5, "$"),
                ("Peter Luger Steak House", "Churrascaria", 4.5, "$$$")
            ],
            'Paris': [
                ("L'Ambroisie", "Francesa", 4.5, "$$$$"),
                ("L'Astrance", "Francesa", 4.5, "$$$$"),
                ("Alain Ducasse Plaza Ath√©n√©e", "Francesa", 4.5, "$$$$"),
                ("Le Meurice", "Francesa", 4.5, "$$$$"),
                ("L'Atelier Saint-Germain", "Francesa", 4.0, "$$$"),
                ("Septime", "Francesa", 4.5, "$$"),
                ("Frenchie", "Francesa", 4.5, "$$"),
                ("Le Comptoir du Relais", "Francesa", 4.0, "$$"),
                ("Bouillon Chartier", "Francesa", 4.0, "$"),
                ("Ladur√©e", "Caf√©", 4.0, "$$")
            ],
            'S√£o Paulo': [
                ("D.O.M.", "Brasileira", 4.5, "$$$$"),
                ("Fasano", "Italiana", 4.5, "$$$$"),
                ("Mani", "Brasileira", 4.5, "$$$"),
                ("Tuju", "Brasileira", 4.5, "$$$"),
                ("Evvai", "Italiana", 4.5, "$$$"),
                ("A Casa do Porco", "Brasileira", 4.5, "$$"),
                ("Figueira Rubaiyat", "Brasileira", 4.5, "$$$"),
                ("Tordesilhas", "Brasileira", 4.0, "$$"),
                ("Pobre Juan", "Argentina", 4.5, "$$"),
                ("Jardin de Winter", "Francesa", 4.0, "$$$")
            ],
            'T√≥quio': [
                ("Sukiyabashi Jiro", "Sushi", 4.5, "$$$$"),
                ("Kanda", "Japonesa", 4.5, "$$$$"),
                ("Narisawa", "Japonesa", 4.5, "$$$$"),
                ("Ryugin", "Japonesa", 4.5, "$$$$"),
                ("Den", "Japonesa", 4.5, "$$$"),
                ("Floril√®ge", "Francesa", 4.5, "$$$"),
                ("Sushi Saito", "Sushi", 4.5, "$$$$"),
                ("Ishikawa", "Japonesa", 4.5, "$$$"),
                ("Kozue", "Japonesa", 4.5, "$$$"),
                ("New York Grill", "Americana", 4.5, "$$$")
            ]
        }

        restaurants = []
        if city_name in city_data:
            for nome, cozinha, avaliacao, preco in city_data[city_name]:
                restaurant = {
                    'nome': nome,
                    'cidade': city_name,
                    'pais': self.get_country_by_city(city_name),
                    'cozinha': cozinha,
                    'avaliacao_tripadvisor': avaliacao,
                    'faixa_preco': preco,
                    'fonte': f'TripAdvisor {city_name}',
                    'data_coleta': datetime.now().strftime('%Y-%m-%d'),
                    'categoria': self.get_category_by_cuisine(cozinha)
                }
                restaurants.append(restaurant)

        return restaurants

    def get_country_by_city(self, city_name):
        """Retorna o pa√≠s baseado na cidade"""
        country_map = {
            'Nova York': 'EUA', 'Paris': 'Fran√ßa', 'Londres': 'Reino Unido',
            'T√≥quio': 'Jap√£o', 'Roma': 'It√°lia', 'Barcelona': 'Espanha',
            'Dubai': 'Emirados √Årabes', 'Singapura': 'Singapura',
            'Hong Kong': 'China', 'Bangkok': 'Tail√¢ndia', 'Sydney': 'Austr√°lia',
            'Rio de Janeiro': 'Brasil', 'S√£o Paulo': 'Brasil',
            'Cidade do M√©xico': 'M√©xico', 'Lisboa': 'Portugal'
        }
        return country_map.get(city_name, 'Desconhecido')

    def get_category_by_cuisine(self, cozinha):
        """Categoriza o restaurante baseado no tipo de cozinha"""
        fine_dining = ['Francesa', 'Italiana', 'Japonesa', 'Brasileira', 'Americana']
        casual = ['Deli', 'Caf√©', 'Argentina']

        if cozinha in fine_dining:
            return 'Fine Dining'
        elif cozinha in casual:
            return 'Casual'
        else:
            return 'Especialidade'

    def scrape_local_guides(self):
        """Coleta dados de guias locais e regionais"""
        logger.info("üìö Coletando de guias locais e regionais...")

        local_restaurants = []

        # Guias regionais
        regional_guides = [
            ('Asia 50 Best', [
                ("Odette", "Singapura", "Julien Royer"),
                ("The Chairman", "Hong Kong", "Danny Yip"),
                ("Narisawa", "T√≥quio", "Yoshihiro Narisawa"),
                ("Den", "T√≥quio", "Zaiyu Hasegawa"),
                ("Gaggan Anand", "Bangkok", "Gaggan Anand"),
                ("S√ºhring", "Bangkok", "Mathias e Thomas S√ºhring"),
                ("Mingles", "Seul", "Mingoo Kang"),
                ("Nae:um", "Seul", "Louis Han"),
                ("Labyrinth", "Singapura", "LG Han"),
                ("Burnt Ends", "Singapura", "Dave Pynt")
            ]),
            ('Latin America 50 Best', [
                ("Central", "Lima", "Virgilio Mart√≠nez"),
                ("Don Julio", "Buenos Aires", "Pablo Rivero"),
                ("Maido", "Lima", "Mitsuharu Tsumura"),
                ("Pujol", "Cidade do M√©xico", "Enrique Olvera"),
                ("Borag√≥", "Santiago", "Rodolfo Guzm√°n"),
                ("A Casa do Porco", "S√£o Paulo", "Jefferson Rueda"),
                ("Quintonil", "Cidade do M√©xico", "Jorge Vallejo"),
                ("Mani", "S√£o Paulo", "Helena Rizzo"),
                ("Osso", "Lima", "Renato Peralta"),
                ("Harry Sasson", "Bogot√°", "Harry Sasson")
            ])
        ]

        for guide_name, restaurants_list in regional_guides:
            for pos, (nome, cidade, chef) in enumerate(restaurants_list, 1):
                restaurant = {
                    'nome': nome,
                    'posicao_ranking': pos,
                    'cidade': cidade,
                    'pais': self.get_country_by_city(cidade),
                    'chef': chef,
                    'fonte': guide_name,
                    'data_coleta': datetime.now().strftime('%Y-%m-%d'),
                    'categoria': 'Fine Dining'
                }
                local_restaurants.append(restaurant)

        return local_restaurants

    def collect_all_data(self):
        """Coleta dados de todas as fontes"""
        logger.info("üöÄ INICIANDO COLETA GLOBAL DE RESTAURANTES")

        all_restaurants = []

        # Coletar de m√∫ltiplas fontes em paralelo
        sources = [
            self.scrape_world_50best_archive,
            self.scrape_michelin_global,
            self.scrape_tripadvisor_global,
            self.scrape_local_guides
        ]

        for source in sources:
            try:
                restaurants = source()
                all_restaurants.extend(restaurants)
                logger.info(f"‚úÖ {source.__name__}: {len(restaurants)} restaurantes")
                time.sleep(1)
            except Exception as e:
                logger.error(f"‚ùå Erro em {source.__name__}: {e}")

        # Remover duplicatas
        unique_restaurants = self.remove_duplicates(all_restaurants)

        logger.info(f"üéØ COLETA CONCLU√çDA: {len(unique_restaurants)} restaurantes √∫nicos")
        return unique_restaurants

    def remove_duplicates(self, restaurants):
        """Remove restaurantes duplicados baseado no nome e cidade"""
        seen = set()
        unique = []

        for rest in restaurants:
            key = (rest['nome'].lower(), rest['cidade'].lower())
            if key not in seen:
                seen.add(key)
                unique.append(rest)

        return unique

    def save_comprehensive_data(self, restaurants):
        """Salva dados completos em m√∫ltiplos formatos"""
        if not restaurants:
            logger.warning("Nenhum dado para salvar")
            return

        df = pd.DataFrame(restaurants)

        # CSV
        df.to_csv('restaurantes_global.csv', index=False, encoding='utf-8')

        # JSON
        with open('restaurantes_global.json', 'w', encoding='utf-8') as f:
            json.dump(restaurants, f, ensure_ascii=False, indent=2)

        # Excel
        df.to_excel('restaurantes_global.xlsx', index=False)

        # Estat√≠sticas
        logger.info(f"üíæ Dados salvos: {len(restaurants)} restaurantes")
        logger.info(f"üåç Pa√≠ses: {df['pais'].nunique()} pa√≠ses diferentes")
        logger.info(f"üèôÔ∏è Cidades: {df['cidade'].nunique()} cidades diferentes")
        logger.info(f"üìä Fontes: {df['fonte'].value_counts().to_dict()}")


def main():
    """Fun√ß√£o principal"""
    scraper = GlobalRestaurantScraper()

    # Coletar dados
    restaurants = scraper.collect_all_data()

    # Salvar dados
    scraper.save_comprehensive_data(restaurants)

    # Relat√≥rio final
    print("\n" + "=" * 70)
    print("üéâ COLETA GLOBAL DE RESTAURANTES CONCLU√çDA!")
    print("=" * 70)

    df = pd.DataFrame(restaurants)
    print(f"üìà ESTAT√çSTICAS FINAIS:")
    print(f"   ‚Ä¢ Total de Restaurantes: {len(restaurants)}")
    print(f"   ‚Ä¢ Pa√≠ses Diferentes: {df['pais'].nunique()}")
    print(f"   ‚Ä¢ Cidades Diferentes: {df['cidade'].nunique()}")
    print(f"   ‚Ä¢ Fontes de Dados: {len(df['fonte'].unique())}")

    print(f"\nüèÜ TOP 5 PA√çSES:")
    print(df['pais'].value_counts().head(5))

    print(f"\nüìÅ ARQUIVOS GERADOS:")
    print("   - restaurantes_global.csv")
    print("   - restaurantes_global.json")
    print("   - restaurantes_global.xlsx")

    print(f"\nüçΩÔ∏è  AMOSTRA DE RESTAURANTES:")
    sample = df[['nome', 'cidade', 'pais', 'fonte']].head(10)
    print(sample.to_string(index=False))


if __name__ == "__main__":
    main()